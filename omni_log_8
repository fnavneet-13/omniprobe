  ___                  _                 _          
 / _ \ _ __ ___  _ __ (_)_ __  _ __ ___ | |__   ___ 
| | | | '_ ` _ \| '_ \| | '_ \| '__/ _ \| '_ \ / _ \
| |_| | | | | | | | | | | |_) | | | (_) | |_) |  __/
 \___/|_| |_| |_|_| |_|_| .__/|_|  \___/|_.__/ \___|
                        |_|                         

Found config file at /workspace/app/omniprobe/build/logDuration/bin/logDuration/runtime_config.txt

Omniprobe is developed by Advanced Micro Devices, Research and Advanced Development
Copyright (c) 2025 Advanced Micro Devices. All rights reserved.

Triton cache location provided; assuming Triton run.
INFO 10-11 22:36:11 [__init__.py:241] Automatically detected platform rocm.
hsaInterceptor: Will start logging at dispatch count: 1
HANDLER: libLogMessages64.so
handlerManager: OpenedlibLogMessages64.so
Added /root/.triton/cache/Nbvtvu7iNVPURIGkGJws1rEOT25xN7hxv29BePCoAa8 to watch list
Added /root/.triton/cache/uqfbcM-KWsr-nRc22lps66AxV4xqVHQ2_ZnFo0ZDd4I to watch list
Added /root/.triton/cache/g3MoZFIMjMexBSUYUvHR2UldlSoEFZ_83SbFugoXqyQ to watch list
Added /root/.triton/cache/aa428699d55a09f96ac3edb5277b75549b42371216c1588af0953c1f90506a87 to watch list
Added /root/.triton/cache/4e1ac38f1261ee221a6db00b2b8443ccb13075d5896e80ba4f1aced69eab43c8 to watch list
Added /root/.triton/cache/9zF3xCLhXPwDgggm6zo6hZs-VFCZFff8FuNeXRJmDsQ to watch list
Added /root/.triton/cache/djes3BprKh3x2Uzz9CnYn8sZXdG0EtA0Px_4PoLwXC0 to watch list
Added /root/.triton/cache/srZ-e8CMW1CtlonHSat61k-JZndT5lYcI2FISAujjuE to watch list
Added /root/.triton/cache/vt3Tvk2p4TIoXHyGYuc9XoMJq8W1p_pqAxDY7huIBdA to watch list
Added /root/.triton/cache/2AvQgiAlT3RdrTQMXJ0ydiqlHlj_bt362-xSVHEPUY0 to watch list
Added /root/.triton/cache/EpBrX9uaV8NYBrUD4ZHgpz15JX08vF1YEfCe59k-Vqw to watch list
Added /root/.triton/cache/7ab3cb8f01f40d2b26874ec9e158d9157e3e55e8d1befc91481eb429f7cf00e5 to watch list
Added /root/.triton/cache/cXAXq7acbwe8tAZ3dplI60g2zJNveIo4g66nnckjyIs to watch list
Added /root/.triton/cache/Anye4sOrXti6ojvldValedZxeWYfyUs6tVwEGW5Z-v4 to watch list
Added /root/.triton/cache/Bx7p2dgwX0lWfV9kV4-SCf_8IfXkBzal4T2Tv71VObY to watch list
Added /root/.triton/cache/3230bd06da152a94d9ed3580336952061f2876d18238eb16d11cd71cbeaadbaf to watch list
Added /root/.triton/cache/6ae4a2076faaa486687f0f4b4cc40104fef727c964d813098f7f4e92e591ec2d to watch list
Added /root/.triton/cache/TymR-F0VOotktHVeBsfZjpPrJvQb-pmVf1vnVh2qmLU to watch list
Added /root/.triton/cache/LesB6mztTSu-KOR7IUmwpHfHTzoRQw3hCk4PgJJ5NwY to watch list
Added /root/.triton/cache/G_aDjs8OdsqAcZ7Xisv8EFT2J8eZnllw2pv1MxNmi3U to watch list
Added /root/.triton/cache/uYV8y7R3Yb6CuVtuQ_VlHf1YEH1qTDUZZQWHe2uV0NU to watch list
Added /root/.triton/cache/EVhVJH3nKAmNOe4qse-eOLR9tL8AUmojkbeMOOrlbJM to watch list
Added /root/.triton/cache/AN_ibmPVwNuzPuPMjO63HFVV0lD3A7a83VMiK08siy8 to watch list
Added /root/.triton/cache/hj-DHZ0yE-CkPZaKYe_x0a-WvuLyBgsB4kFiwICb2r8 to watch list
Added /root/.triton/cache/PYn0oU4EI9fgaAnSfxxhd-4-qlwjvmJ-I9gOzOCIGrY to watch list
Added /root/.triton/cache/M5JSQ1TNvWRDGzbi7ymLNLIQf5DA61d3uSXGAJahJjo to watch list
Added /root/.triton/cache/b1b4e3e72779a0aeccf62c269260ca797c6f78f5da25e998d7602aa06de76c9b to watch list
Added /root/.triton/cache/IlyGmbqfnPUTKV02TQRiu_dNNecWegkJ8_8B4eNM1MU to watch list
Added /root/.triton/cache/chQ8kgtGgqQyccrEuC-PHSaLb6iN5Jee7rr3DPBHh2s to watch list
Added /root/.triton/cache/dDZetgVQGqKpurYGW4kjN--EM9O-Mp9TciMERMgkygo to watch list
Added /root/.triton/cache/d1ruhrUr7FBBqH90AofYWBUURV69YGIbIAIeb_nIWZQ to watch list
Added /root/.triton/cache/sAb5QqqcqW3ymoYkfHDwBLk6VAQbgku9pcgphzFBuCE to watch list
Added /root/.triton/cache/LWBORQb-wjngsEQ_IFuw79TNVEjzzD236LZXg3oy7P0 to watch list
Added /root/.triton/cache/2J0RFEU5Z2DXTN8vsiWcl74OSLx4VF54DQEyoKhtKAI to watch list
Added /root/.triton/cache/c9620a12405646d323f4f8e2ff7b4ed7a536e14c5bcd84cb8c043e421ca6c023 to watch list
Added /root/.triton/cache/a2c1d67df5eccf652a5b8730732e112c91ed8bbd21a0d72fdb9b24d7902c1772 to watch list
Added /root/.triton/cache/Jk9ucPmtUo4qKuEggnAq9p6Nq43NgZsyoVINHUiqoR8 to watch list
Added /root/.triton/cache/efce399b1d0635783112b4fe410e0f2010ee7f97455f56265a34214ac5cb3a2a to watch list
Added /root/.triton/cache/ZWuPcAKfnrC3RjfZGeE073lgB8Veeq5KXvDJ6sqTI5Q to watch list
Added /root/.triton/cache/tVEzUx3QNg8OyRsxJnAeX9Ng_7O-0TuKbTUMB8I4_8M to watch list
Added /root/.triton/cache/a3LBQB_CfXIFZrjdYhOePkGu93ijpMb9xmOy-xHiGXk to watch list
Added /root/.triton/cache/oCMqPJQH3uBtoD0E5Uun_FqlqcXLpU2LFeR2bciZslI to watch list
Added /root/.triton/cache/mxLOzGdSTzD_ukj6pv-vYyLU8Oi0V7TGV8qkPwrxurA to watch list
Added /root/.triton/cache/es8uicp2E1mxITjETc4XIEaV5t9medU-3lNQDlBQvcs to watch list
Added /root/.triton/cache/g3UmD1y-NRHjvqKPrGBcYaF279HjssP943kq9hlCvxA to watch list
Added /root/.triton/cache/_d3689EZZOEc6xP4HqVCj2XOe8NMMrxTsdMSePzgI3I to watch list
Added /root/.triton/cache/-nKif9bcaRQmIdSJY80PVGW0HE3ZfvDZVM1Rl5P6OFI to watch list
Added /root/.triton/cache/CT9q49r-ZaSexPZWH2yFRhwBDElRvwIDtlfwNjjnZnM to watch list
Added /root/.triton/cache/IrFrfKU8qFFV8ytkp8FiOt1umNgbGvd5DYZIvwUhnfg to watch list
Added /root/.triton/cache/lL0nyNovJnCLXYGBkF4yqm0UTiDe1jtfx3xpNuZkxwA to watch list
Added /root/.triton/cache/46JrxFn2BoGTWXuCmcQ5fXrMYRlaWEmMZB5genteubs to watch list
Added /root/.triton/cache/NSWtE148SN7CZU0uD_X0bnFCBJG53f20Oba4SX5LYwg to watch list
Added /root/.triton/cache/RA8GpRb97Jda70bVIlIArlkU5bsPDfz6BfZjyGcPSq0 to watch list
Added /root/.triton/cache/i_CHJSP1As2HevzWUrsdFfmV_uD6syw4WdzznULsoQo to watch list
Added /root/.triton/cache/vJ5gIhstBH6KqX2sMmd99vQcjg_iVyrvlHeEdeAZAC0 to watch list
Added /root/.triton/cache/5L_rxSY4TC8C8FQh5CJV26wl8KE5l6xsNExuqrPLumY to watch list
Added /root/.triton/cache/tb1EpkkSlCpudixttgu9BmhD-bw5z0fYtKU8eFGGBRA to watch list
Added /root/.triton/cache/e5a84e1d3d76f88e282cd4bdada70e86679c3fc820dbd966f787c9894b34d399 to watch list
Added /root/.triton/cache/4d4c96178d018b6c92a7370322cfd9a0426171bf3d21c7ae807faa9f82819242 to watch list
Added /root/.triton/cache/9b1d11b8845de7ca5deeab0626bbc7289dafeca07d021496f039976492e2b8e0 to watch list
Added /root/.triton/cache/XuW8pCLW78qfO5jNQmMz-28_53jNcaJq6SKXn0ofAyo to watch list
Added /root/.triton/cache/yCvIOg57qhGaKhsd0YKGYpuEarEL4ZJu8v_tL4ZbMwc to watch list
Added /root/.triton/cache/LBbarmtBORb98MUirV2O2lYUqIoClQ5cCuZT0eiTIg4 to watch list
Added /root/.triton/cache/OCBGV7y6zTPamQsRlzTeGszFucjrjXBArvFgM2vZA_I to watch list
Added /root/.triton/cache/snIGq7COu7tYWETNlsM9k5lLPPtWE-csVcy31G6kLHg to watch list
Added /root/.triton/cache/72fdcb094a4292bcfe800556da6bdc71ed61471b3cbc9fa44dd81f924f150e56 to watch list
Added /root/.triton/cache/pXe7YspcfBqsYOVFmMPtS-Fsi4Bjp0GbGES6zF5BIw4 to watch list
Added /root/.triton/cache/dXmKFlghsepqpCicsyho7c4tMcYaJwP9jXXXGyVjbzk to watch list
Added /root/.triton/cache/8BLfPrX9s_4eYwJTyyWT9nqzeMFqXz1VA-AVu9KEhy4 to watch list
Added /root/.triton/cache/W33ETNiSrnZQ9b3FxwFgLjks-6o4upMWrcJNRRF5gso to watch list
Added /root/.triton/cache/SDTN_quGP-G1OLugU_w2IyZw7L3CgrKZGjqf8rwNzUU to watch list
Added /root/.triton/cache/_aoJ5vptcriqLkMHj_Kw2Vp17RyE2DT1OAK_LQU9d0E to watch list
Added /root/.triton/cache/ddfQEOnNCT8rqC_2mhX6vOl89Bc1AafeP-2P8e0Qxl0 to watch list
Added /root/.triton/cache/Ib-3tYjF-og-JA6tzBxj-kicBJpN5vVf21BVnLir4pM to watch list
Added /root/.triton/cache/ZP0ZJUdjR6u78x4PDIWdTy1ynkyH4uTP4Z9qhv4rDnQ to watch list
Added /root/.triton/cache/yA_X5RZKMs7FMCnehGvXbQYqKXtGWTw-VokFOgZyths to watch list
Added /root/.triton/cache/veHciKmuAWKsqCMSPH7Sxz69bfW9m0KbZ6WYqive_JQ to watch list
Added /root/.triton/cache/GHH85z-V2hD6dOv06O1cGCAdMVyMlSvpyhzjIUuUdU8 to watch list
Added /root/.triton/cache/FSYE3oK8kk2wPGZaSI5gWJ-8Xovcx4tdxEmBbJCNmn8 to watch list
Added /root/.triton/cache/jG4guBv9gnHFTsFgAiJM_DKGA_8LMlBhZBzsMpKTbfc to watch list
Added /root/.triton/cache/duwpwttZ0jEuhTdgjChSUhT7Z3RaefrasZXCta7_Yf0 to watch list
Added /root/.triton/cache/719bd61bf2c8283fdb74f051e4681a1a7a5bac50dcbcdeadc80ec8329a20a4e8 to watch list
Added /root/.triton/cache/6KrJHckisCPLpyE8h5rizfKSwp_9gxj86synn1l9HDc to watch list
Added /root/.triton/cache/RfU7bLdfxK8rEzFmjx_8u4HXL-voxgaO6G2B5QedrF4 to watch list
Added /root/.triton/cache/-GYL5LvZzLc6_fARZFZVDBuDWPxrc4NyZulAUPCh69M to watch list
Added /root/.triton/cache/rL_l0AB-4KSKOGnU5ukD5rtW2GHQKdz-6Z1KwDQkIJA to watch list
Added /root/.triton/cache/PiOSQnoWZrADV4dRSti7hBH9QjpXc-70kIcI7b2BH9M to watch list
Added /root/.triton/cache/SHWWderabTck9-IkN-DylXHPEGwBsTE3URH3qeS3AJY to watch list
Added /root/.triton/cache/uS56_G-aDV4wkvaqN2w76NRU2ZyXksUbi3tW0tJn_Uw to watch list
Added /root/.triton/cache/QP-iX0auCt4S5OZDkZjuLriE6H53zRIoLqBrRWS5IiQ to watch list
Added /root/.triton/cache/8vNHA81U-ewNfPMung7TUUxBPgHLM0ca6bAkq4pjIco to watch list
Added /root/.triton/cache/6VMv4IXqSL-4tgHSTmxh2FH0S5xeH-rSiuBRMVrtdJ8 to watch list
Added /root/.triton/cache/8f29e025e3bf6dc5af145d69108da996942b762a4d66fab70e7958508d702016 to watch list
Added /root/.triton/cache/PWFkVX5jfJEhcp-zfwX5OGQFxDfYPP6HK3ptfLwUKD8 to watch list
Added /root/.triton/cache/z4dTb2NWzzl0mfaE-R9WohrrecP738aEalLWB8a98Qg to watch list
Added /root/.triton/cache/mfjqnW6noiW4s3zIaPlu23dMFW6xYV2ukimhkO4AIgw to watch list
Added /root/.triton/cache/4M3iSteYjJ4lUUORER4Lh1wAGBLI4XNQTn4mbCF1EVk to watch list
Added /root/.triton/cache/A5k7aNm3l4B1VDKCKumftezvfGK6FP5LMKMtnO8wQmc to watch list
Added /root/.triton/cache/WJKBknLLrybO-QQZ7WhOmAocNHfCdjWaM_JENliy-nQ to watch list
Added /root/.triton/cache/iKTtcLgtJqcYbBkgOLkv0wwoXAn8RRdRlmsnGjhEDbY to watch list
INFO 10-11 22:36:17 [utils.py:326] non-default args: {'model': '/models/meta-llama/Llama-3.1-8B-Instruct/', 'disable_log_stats': True}
INFO 10-11 22:36:28 [__init__.py:742] Resolved architecture: LlamaForCausalLM
INFO 10-11 22:36:28 [__init__.py:1774] Using max model len 131072
INFO 10-11 22:36:28 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 10-11 22:36:28 [__init__.py:2945] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
INFO 10-11 22:36:32 [__init__.py:241] Automatically detected platform rocm.
hsaInterceptor: Will start logging at dispatch count: 1
HANDLER: libLogMessages64.so
handlerManager: OpenedlibLogMessages64.so
Added /root/.triton/cache/Nbvtvu7iNVPURIGkGJws1rEOT25xN7hxv29BePCoAa8 to watch list
Added /root/.triton/cache/uqfbcM-KWsr-nRc22lps66AxV4xqVHQ2_ZnFo0ZDd4I to watch list
Added /root/.triton/cache/g3MoZFIMjMexBSUYUvHR2UldlSoEFZ_83SbFugoXqyQ to watch list
Added /root/.triton/cache/aa428699d55a09f96ac3edb5277b75549b42371216c1588af0953c1f90506a87 to watch list
Added /root/.triton/cache/4e1ac38f1261ee221a6db00b2b8443ccb13075d5896e80ba4f1aced69eab43c8 to watch list
Added /root/.triton/cache/9zF3xCLhXPwDgggm6zo6hZs-VFCZFff8FuNeXRJmDsQ to watch list
Added /root/.triton/cache/djes3BprKh3x2Uzz9CnYn8sZXdG0EtA0Px_4PoLwXC0 to watch list
Added /root/.triton/cache/srZ-e8CMW1CtlonHSat61k-JZndT5lYcI2FISAujjuE to watch list
Added /root/.triton/cache/vt3Tvk2p4TIoXHyGYuc9XoMJq8W1p_pqAxDY7huIBdA to watch list
Added /root/.triton/cache/2AvQgiAlT3RdrTQMXJ0ydiqlHlj_bt362-xSVHEPUY0 to watch list
Added /root/.triton/cache/EpBrX9uaV8NYBrUD4ZHgpz15JX08vF1YEfCe59k-Vqw to watch list
Added /root/.triton/cache/7ab3cb8f01f40d2b26874ec9e158d9157e3e55e8d1befc91481eb429f7cf00e5 to watch list
Added /root/.triton/cache/cXAXq7acbwe8tAZ3dplI60g2zJNveIo4g66nnckjyIs to watch list
Added /root/.triton/cache/Anye4sOrXti6ojvldValedZxeWYfyUs6tVwEGW5Z-v4 to watch list
Added /root/.triton/cache/Bx7p2dgwX0lWfV9kV4-SCf_8IfXkBzal4T2Tv71VObY to watch list
Added /root/.triton/cache/3230bd06da152a94d9ed3580336952061f2876d18238eb16d11cd71cbeaadbaf to watch list
Added /root/.triton/cache/6ae4a2076faaa486687f0f4b4cc40104fef727c964d813098f7f4e92e591ec2d to watch list
Added /root/.triton/cache/TymR-F0VOotktHVeBsfZjpPrJvQb-pmVf1vnVh2qmLU to watch list
Added /root/.triton/cache/LesB6mztTSu-KOR7IUmwpHfHTzoRQw3hCk4PgJJ5NwY to watch list
Added /root/.triton/cache/G_aDjs8OdsqAcZ7Xisv8EFT2J8eZnllw2pv1MxNmi3U to watch list
Added /root/.triton/cache/uYV8y7R3Yb6CuVtuQ_VlHf1YEH1qTDUZZQWHe2uV0NU to watch list
Added /root/.triton/cache/EVhVJH3nKAmNOe4qse-eOLR9tL8AUmojkbeMOOrlbJM to watch list
Added /root/.triton/cache/AN_ibmPVwNuzPuPMjO63HFVV0lD3A7a83VMiK08siy8 to watch list
Added /root/.triton/cache/hj-DHZ0yE-CkPZaKYe_x0a-WvuLyBgsB4kFiwICb2r8 to watch list
Added /root/.triton/cache/PYn0oU4EI9fgaAnSfxxhd-4-qlwjvmJ-I9gOzOCIGrY to watch list
Added /root/.triton/cache/M5JSQ1TNvWRDGzbi7ymLNLIQf5DA61d3uSXGAJahJjo to watch list
Added /root/.triton/cache/b1b4e3e72779a0aeccf62c269260ca797c6f78f5da25e998d7602aa06de76c9b to watch list
Added /root/.triton/cache/IlyGmbqfnPUTKV02TQRiu_dNNecWegkJ8_8B4eNM1MU to watch list
Added /root/.triton/cache/chQ8kgtGgqQyccrEuC-PHSaLb6iN5Jee7rr3DPBHh2s to watch list
Added /root/.triton/cache/dDZetgVQGqKpurYGW4kjN--EM9O-Mp9TciMERMgkygo to watch list
Added /root/.triton/cache/d1ruhrUr7FBBqH90AofYWBUURV69YGIbIAIeb_nIWZQ to watch list
Added /root/.triton/cache/sAb5QqqcqW3ymoYkfHDwBLk6VAQbgku9pcgphzFBuCE to watch list
Added /root/.triton/cache/LWBORQb-wjngsEQ_IFuw79TNVEjzzD236LZXg3oy7P0 to watch list
Added /root/.triton/cache/2J0RFEU5Z2DXTN8vsiWcl74OSLx4VF54DQEyoKhtKAI to watch list
Added /root/.triton/cache/c9620a12405646d323f4f8e2ff7b4ed7a536e14c5bcd84cb8c043e421ca6c023 to watch list
Added /root/.triton/cache/a2c1d67df5eccf652a5b8730732e112c91ed8bbd21a0d72fdb9b24d7902c1772 to watch list
Added /root/.triton/cache/Jk9ucPmtUo4qKuEggnAq9p6Nq43NgZsyoVINHUiqoR8 to watch list
Added /root/.triton/cache/efce399b1d0635783112b4fe410e0f2010ee7f97455f56265a34214ac5cb3a2a to watch list
Added /root/.triton/cache/ZWuPcAKfnrC3RjfZGeE073lgB8Veeq5KXvDJ6sqTI5Q to watch list
Added /root/.triton/cache/tVEzUx3QNg8OyRsxJnAeX9Ng_7O-0TuKbTUMB8I4_8M to watch list
Added /root/.triton/cache/a3LBQB_CfXIFZrjdYhOePkGu93ijpMb9xmOy-xHiGXk to watch list
Added /root/.triton/cache/oCMqPJQH3uBtoD0E5Uun_FqlqcXLpU2LFeR2bciZslI to watch list
Added /root/.triton/cache/mxLOzGdSTzD_ukj6pv-vYyLU8Oi0V7TGV8qkPwrxurA to watch list
Added /root/.triton/cache/es8uicp2E1mxITjETc4XIEaV5t9medU-3lNQDlBQvcs to watch list
Added /root/.triton/cache/g3UmD1y-NRHjvqKPrGBcYaF279HjssP943kq9hlCvxA to watch list
Added /root/.triton/cache/_d3689EZZOEc6xP4HqVCj2XOe8NMMrxTsdMSePzgI3I to watch list
Added /root/.triton/cache/-nKif9bcaRQmIdSJY80PVGW0HE3ZfvDZVM1Rl5P6OFI to watch list
Added /root/.triton/cache/CT9q49r-ZaSexPZWH2yFRhwBDElRvwIDtlfwNjjnZnM to watch list
Added /root/.triton/cache/IrFrfKU8qFFV8ytkp8FiOt1umNgbGvd5DYZIvwUhnfg to watch list
Added /root/.triton/cache/lL0nyNovJnCLXYGBkF4yqm0UTiDe1jtfx3xpNuZkxwA to watch list
Added /root/.triton/cache/46JrxFn2BoGTWXuCmcQ5fXrMYRlaWEmMZB5genteubs to watch list
Added /root/.triton/cache/NSWtE148SN7CZU0uD_X0bnFCBJG53f20Oba4SX5LYwg to watch list
Added /root/.triton/cache/RA8GpRb97Jda70bVIlIArlkU5bsPDfz6BfZjyGcPSq0 to watch list
Added /root/.triton/cache/i_CHJSP1As2HevzWUrsdFfmV_uD6syw4WdzznULsoQo to watch list
Added /root/.triton/cache/vJ5gIhstBH6KqX2sMmd99vQcjg_iVyrvlHeEdeAZAC0 to watch list
Added /root/.triton/cache/5L_rxSY4TC8C8FQh5CJV26wl8KE5l6xsNExuqrPLumY to watch list
Added /root/.triton/cache/tb1EpkkSlCpudixttgu9BmhD-bw5z0fYtKU8eFGGBRA to watch list
Added /root/.triton/cache/e5a84e1d3d76f88e282cd4bdada70e86679c3fc820dbd966f787c9894b34d399 to watch list
Added /root/.triton/cache/4d4c96178d018b6c92a7370322cfd9a0426171bf3d21c7ae807faa9f82819242 to watch list
Added /root/.triton/cache/9b1d11b8845de7ca5deeab0626bbc7289dafeca07d021496f039976492e2b8e0 to watch list
Added /root/.triton/cache/XuW8pCLW78qfO5jNQmMz-28_53jNcaJq6SKXn0ofAyo to watch list
Added /root/.triton/cache/yCvIOg57qhGaKhsd0YKGYpuEarEL4ZJu8v_tL4ZbMwc to watch list
Added /root/.triton/cache/LBbarmtBORb98MUirV2O2lYUqIoClQ5cCuZT0eiTIg4 to watch list
Added /root/.triton/cache/OCBGV7y6zTPamQsRlzTeGszFucjrjXBArvFgM2vZA_I to watch list
Added /root/.triton/cache/snIGq7COu7tYWETNlsM9k5lLPPtWE-csVcy31G6kLHg to watch list
Added /root/.triton/cache/72fdcb094a4292bcfe800556da6bdc71ed61471b3cbc9fa44dd81f924f150e56 to watch list
Added /root/.triton/cache/pXe7YspcfBqsYOVFmMPtS-Fsi4Bjp0GbGES6zF5BIw4 to watch list
Added /root/.triton/cache/dXmKFlghsepqpCicsyho7c4tMcYaJwP9jXXXGyVjbzk to watch list
Added /root/.triton/cache/8BLfPrX9s_4eYwJTyyWT9nqzeMFqXz1VA-AVu9KEhy4 to watch list
Added /root/.triton/cache/W33ETNiSrnZQ9b3FxwFgLjks-6o4upMWrcJNRRF5gso to watch list
Added /root/.triton/cache/SDTN_quGP-G1OLugU_w2IyZw7L3CgrKZGjqf8rwNzUU to watch list
Added /root/.triton/cache/_aoJ5vptcriqLkMHj_Kw2Vp17RyE2DT1OAK_LQU9d0E to watch list
Added /root/.triton/cache/ddfQEOnNCT8rqC_2mhX6vOl89Bc1AafeP-2P8e0Qxl0 to watch list
Added /root/.triton/cache/Ib-3tYjF-og-JA6tzBxj-kicBJpN5vVf21BVnLir4pM to watch list
Added /root/.triton/cache/ZP0ZJUdjR6u78x4PDIWdTy1ynkyH4uTP4Z9qhv4rDnQ to watch list
Added /root/.triton/cache/yA_X5RZKMs7FMCnehGvXbQYqKXtGWTw-VokFOgZyths to watch list
Added /root/.triton/cache/veHciKmuAWKsqCMSPH7Sxz69bfW9m0KbZ6WYqive_JQ to watch list
Added /root/.triton/cache/GHH85z-V2hD6dOv06O1cGCAdMVyMlSvpyhzjIUuUdU8 to watch list
Added /root/.triton/cache/FSYE3oK8kk2wPGZaSI5gWJ-8Xovcx4tdxEmBbJCNmn8 to watch list
Added /root/.triton/cache/jG4guBv9gnHFTsFgAiJM_DKGA_8LMlBhZBzsMpKTbfc to watch list
Added /root/.triton/cache/duwpwttZ0jEuhTdgjChSUhT7Z3RaefrasZXCta7_Yf0 to watch list
Added /root/.triton/cache/719bd61bf2c8283fdb74f051e4681a1a7a5bac50dcbcdeadc80ec8329a20a4e8 to watch list
Added /root/.triton/cache/6KrJHckisCPLpyE8h5rizfKSwp_9gxj86synn1l9HDc to watch list
Added /root/.triton/cache/RfU7bLdfxK8rEzFmjx_8u4HXL-voxgaO6G2B5QedrF4 to watch list
Added /root/.triton/cache/-GYL5LvZzLc6_fARZFZVDBuDWPxrc4NyZulAUPCh69M to watch list
Added /root/.triton/cache/rL_l0AB-4KSKOGnU5ukD5rtW2GHQKdz-6Z1KwDQkIJA to watch list
Added /root/.triton/cache/PiOSQnoWZrADV4dRSti7hBH9QjpXc-70kIcI7b2BH9M to watch list
Added /root/.triton/cache/SHWWderabTck9-IkN-DylXHPEGwBsTE3URH3qeS3AJY to watch list
Added /root/.triton/cache/uS56_G-aDV4wkvaqN2w76NRU2ZyXksUbi3tW0tJn_Uw to watch list
Added /root/.triton/cache/QP-iX0auCt4S5OZDkZjuLriE6H53zRIoLqBrRWS5IiQ to watch list
Added /root/.triton/cache/8vNHA81U-ewNfPMung7TUUxBPgHLM0ca6bAkq4pjIco to watch list
Added /root/.triton/cache/6VMv4IXqSL-4tgHSTmxh2FH0S5xeH-rSiuBRMVrtdJ8 to watch list
Added /root/.triton/cache/8f29e025e3bf6dc5af145d69108da996942b762a4d66fab70e7958508d702016 to watch list
Added /root/.triton/cache/PWFkVX5jfJEhcp-zfwX5OGQFxDfYPP6HK3ptfLwUKD8 to watch list
Added /root/.triton/cache/z4dTb2NWzzl0mfaE-R9WohrrecP738aEalLWB8a98Qg to watch list
Added /root/.triton/cache/mfjqnW6noiW4s3zIaPlu23dMFW6xYV2ukimhkO4AIgw to watch list
Added /root/.triton/cache/4M3iSteYjJ4lUUORER4Lh1wAGBLI4XNQTn4mbCF1EVk to watch list
Added /root/.triton/cache/A5k7aNm3l4B1VDKCKumftezvfGK6FP5LMKMtnO8wQmc to watch list
Added /root/.triton/cache/WJKBknLLrybO-QQZ7WhOmAocNHfCdjWaM_JENliy-nQ to watch list
Added /root/.triton/cache/iKTtcLgtJqcYbBkgOLkv0wwoXAn8RRdRlmsnGjhEDbY to watch list
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:37 [core.py:644] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:37 [core.py:74] Initializing a V1 LLM engine (v0.10.1rc2.dev155+gebe14621e) with config: model='/models/meta-llama/Llama-3.1-8B-Instruct/', speculative_config=None, tokenizer='/models/meta-llama/Llama-3.1-8B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/models/meta-llama/Llama-3.1-8B-Instruct/, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":3,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output","vllm.mamba_mixer2","vllm.mamba_mixer","vllm.short_conv"],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":1,"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":512,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:39 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
>>>>>>>> HSA intercept registered.
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:39 [gpu_model_runner.py:1966] Starting to load model /models/meta-llama/Llama-3.1-8B-Instruct/...
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:40 [gpu_model_runner.py:1998] Loading model from scratch...
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:40 [rocm.py:245] Using Triton Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:40 [triton_attn.py:258] Using vllm unified attention for TritonAttentionImpl
[1;36m(EngineCore_0 pid=45884)[0;0m
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(EngineCore_0 pid=45884)[0;0m
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.81it/s]
[1;36m(EngineCore_0 pid=45884)[0;0m
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:02<00:02,  1.48s/it]
[1;36m(EngineCore_0 pid=45884)[0;0m
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:05<00:01,  1.89s/it]
[1;36m(EngineCore_0 pid=45884)[0;0m
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:07<00:00,  2.07s/it]
[1;36m(EngineCore_0 pid=45884)[0;0m
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:07<00:00,  1.85s/it]
[1;36m(EngineCore_0 pid=45884)[0;0m
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:48 [default_loader.py:267] Loading weights took 7.71 seconds
[1;36m(EngineCore_0 pid=45884)[0;0m INFO 10-11 22:36:48 [gpu_model_runner.py:2020] Model loading took 15.0759 GiB and 7.909503 seconds
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708] EngineCore failed to start.
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708] Traceback (most recent call last):
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 699, in run_engine_core
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 500, in __init__
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 89, in __init__
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 180, in _initialize_kv_caches
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     self.model_executor.determine_available_memory())
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/v1/executor/abstract.py", line 84, in determine_available_memory
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     output = self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/utils/__init__.py", line 3031, in run_method
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/v1/worker/gpu_worker.py", line 245, in determine_available_memory
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     self.model_runner.profile_run()
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/v1/worker/gpu_model_runner.py", line 2630, in profile_run
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     = self._dummy_run(self.max_num_tokens, is_profile=True)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/v1/worker/gpu_model_runner.py", line 2407, in _dummy_run
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     outputs = self.model(
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/model_executor/models/llama.py", line 719, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     model_output = self.model(input_ids, positions, intermediate_tensors,
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/compilation/decorators.py", line 305, in __call__
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     output = self.compiled_callable(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 659, in _fn
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     raise e.with_traceback(None) from None
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708] torch._dynamo.exc.Unsupported: Unsupported method call
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   Explanation: Dynamo does not know how to trace method `__call__` of class `nb_func`
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   Hint: Avoid calling `nb_func.__call__` in your code.
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   Hint: Please report an issue to PyTorch.
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   Developer debug context: call_method UserDefinedObjectVariable(nb_func) __call__ [ConstantVariable(int: 0)] {}
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708] from user code:
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]    File "/workspace/vllm_hope/vllm/vllm/model_executor/models/llama.py", line 527, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     hidden_states, residual = layer(positions, hidden_states, residual)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/model_executor/models/llama.py", line 440, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     hidden_states = self.self_attn(positions=positions,
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/model_executor/models/llama.py", line 322, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     qkv, _ = self.qkv_proj(hidden_states)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/model_executor/layers/linear.py", line 644, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     output_parallel = self.quant_method.apply(self, input_, bias)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/vllm/vllm/model_executor/layers/linear.py", line 277, in apply
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     tritonblas.matmul(x, layer.weight.t(), output)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/tritonBLAS/include/tritonblas/matmul.py", line 211, in matmul
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     selector = _make_matmul_selector(M, N, K, a.dtype, b.dtype, c.dtype)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py", line 140, in getattr_and_trace
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     return fn(*args[2:], **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/tritonBLAS/include/tritonblas/matmul.py", line 35, in _make_matmul_selector
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     return MatmulHeuristicResult(M, N, K, a_dtype, b_dtype, c_dtype)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py", line 156, in instantiate_user_defined_class_object
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     obj.__init__(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]   File "/workspace/tritonBLAS/include/tritonblas/origami.py", line 41, in __init__
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]     self.hardware = origami.get_hardware_for_device(0)
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708] Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_0 pid=45884)[0;0m ERROR 10-11 22:36:48 [core.py:708]
[1;36m(EngineCore_0 pid=45884)[0;0m Process EngineCore_0:
[1;36m(EngineCore_0 pid=45884)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/opt/conda/envs/py_3.10/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
[1;36m(EngineCore_0 pid=45884)[0;0m     self.run()
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/opt/conda/envs/py_3.10/lib/python3.10/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_0 pid=45884)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 712, in run_engine_core
[1;36m(EngineCore_0 pid=45884)[0;0m     raise e
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 699, in run_engine_core
[1;36m(EngineCore_0 pid=45884)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 500, in __init__
[1;36m(EngineCore_0 pid=45884)[0;0m     super().__init__(vllm_config, executor_class, log_stats,
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 89, in __init__
[1;36m(EngineCore_0 pid=45884)[0;0m     self._initialize_kv_caches(vllm_config)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/engine/core.py", line 180, in _initialize_kv_caches
[1;36m(EngineCore_0 pid=45884)[0;0m     self.model_executor.determine_available_memory())
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/executor/abstract.py", line 84, in determine_available_memory
[1;36m(EngineCore_0 pid=45884)[0;0m     output = self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/executor/uniproc_executor.py", line 58, in collective_rpc
[1;36m(EngineCore_0 pid=45884)[0;0m     answer = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/utils/__init__.py", line 3031, in run_method
[1;36m(EngineCore_0 pid=45884)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_0 pid=45884)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/worker/gpu_worker.py", line 245, in determine_available_memory
[1;36m(EngineCore_0 pid=45884)[0;0m     self.model_runner.profile_run()
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/worker/gpu_model_runner.py", line 2630, in profile_run
[1;36m(EngineCore_0 pid=45884)[0;0m     = self._dummy_run(self.max_num_tokens, is_profile=True)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[1;36m(EngineCore_0 pid=45884)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/v1/worker/gpu_model_runner.py", line 2407, in _dummy_run
[1;36m(EngineCore_0 pid=45884)[0;0m     outputs = self.model(
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
[1;36m(EngineCore_0 pid=45884)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
[1;36m(EngineCore_0 pid=45884)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/model_executor/models/llama.py", line 719, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m     model_output = self.model(input_ids, positions, intermediate_tensors,
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/compilation/decorators.py", line 305, in __call__
[1;36m(EngineCore_0 pid=45884)[0;0m     output = self.compiled_callable(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py", line 659, in _fn
[1;36m(EngineCore_0 pid=45884)[0;0m     raise e.with_traceback(None) from None
[1;36m(EngineCore_0 pid=45884)[0;0m torch._dynamo.exc.Unsupported: Unsupported method call
[1;36m(EngineCore_0 pid=45884)[0;0m   Explanation: Dynamo does not know how to trace method `__call__` of class `nb_func`
[1;36m(EngineCore_0 pid=45884)[0;0m   Hint: Avoid calling `nb_func.__call__` in your code.
[1;36m(EngineCore_0 pid=45884)[0;0m   Hint: Please report an issue to PyTorch.
[1;36m(EngineCore_0 pid=45884)[0;0m
[1;36m(EngineCore_0 pid=45884)[0;0m   Developer debug context: call_method UserDefinedObjectVariable(nb_func) __call__ [ConstantVariable(int: 0)] {}
[1;36m(EngineCore_0 pid=45884)[0;0m
[1;36m(EngineCore_0 pid=45884)[0;0m
[1;36m(EngineCore_0 pid=45884)[0;0m from user code:
[1;36m(EngineCore_0 pid=45884)[0;0m    File "/workspace/vllm_hope/vllm/vllm/model_executor/models/llama.py", line 527, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m     hidden_states, residual = layer(positions, hidden_states, residual)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/model_executor/models/llama.py", line 440, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m     hidden_states = self.self_attn(positions=positions,
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/model_executor/models/llama.py", line 322, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m     qkv, _ = self.qkv_proj(hidden_states)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/model_executor/layers/linear.py", line 644, in forward
[1;36m(EngineCore_0 pid=45884)[0;0m     output_parallel = self.quant_method.apply(self, input_, bias)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/vllm/vllm/model_executor/layers/linear.py", line 277, in apply
[1;36m(EngineCore_0 pid=45884)[0;0m     tritonblas.matmul(x, layer.weight.t(), output)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/tritonBLAS/include/tritonblas/matmul.py", line 211, in matmul
[1;36m(EngineCore_0 pid=45884)[0;0m     selector = _make_matmul_selector(M, N, K, a.dtype, b.dtype, c.dtype)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py", line 140, in getattr_and_trace
[1;36m(EngineCore_0 pid=45884)[0;0m     return fn(*args[2:], **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/tritonBLAS/include/tritonblas/matmul.py", line 35, in _make_matmul_selector
[1;36m(EngineCore_0 pid=45884)[0;0m     return MatmulHeuristicResult(M, N, K, a_dtype, b_dtype, c_dtype)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/vllm_hope/venv_hope/lib/python3.10/site-packages/torch/_dynamo/polyfills/__init__.py", line 156, in instantiate_user_defined_class_object
[1;36m(EngineCore_0 pid=45884)[0;0m     obj.__init__(*args, **kwargs)
[1;36m(EngineCore_0 pid=45884)[0;0m   File "/workspace/tritonBLAS/include/tritonblas/origami.py", line 41, in __init__
[1;36m(EngineCore_0 pid=45884)[0;0m     self.hardware = origami.get_hardware_for_device(0)
[1;36m(EngineCore_0 pid=45884)[0;0m
[1;36m(EngineCore_0 pid=45884)[0;0m Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"
[1;36m(EngineCore_0 pid=45884)[0;0m
[rank0]:[W1011 22:36:49.955850043 ProcessGroupNCCL.cpp:1476] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
signal_runner is shutting down
Comms Runner shutting down
Cache Watcher shutting down
hsaInterceptor: Application elapsed usecs: 16718424us
Traceback (most recent call last):
  File "/workspace/vllm_old/vllm/examples/offline_inference/basic/basic.py", line 38, in <module>
    main()
  File "/workspace/vllm_old/vllm/examples/offline_inference/basic/basic.py", line 21, in main
    llm = LLM(model="/models/meta-llama/Llama-3.1-8B-Instruct/")
  File "/workspace/vllm_hope/vllm/vllm/entrypoints/llm.py", line 270, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/workspace/vllm_hope/vllm/vllm/engine/llm_engine.py", line 490, in from_engine_args
    return engine_cls.from_vllm_config(
  File "/workspace/vllm_hope/vllm/vllm/v1/engine/llm_engine.py", line 127, in from_vllm_config
    return cls(vllm_config=vllm_config,
  File "/workspace/vllm_hope/vllm/vllm/v1/engine/llm_engine.py", line 104, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/workspace/vllm_hope/vllm/vllm/v1/engine/core_client.py", line 80, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/workspace/vllm_hope/vllm/vllm/v1/engine/core_client.py", line 600, in __init__
    super().__init__(
  File "/workspace/vllm_hope/vllm/vllm/v1/engine/core_client.py", line 446, in __init__
    with launch_core_engines(vllm_config, executor_class,
  File "/opt/conda/envs/py_3.10/lib/python3.10/contextlib.py", line 142, in __exit__
    next(self.gen)
  File "/workspace/vllm_hope/vllm/vllm/v1/engine/utils.py", line 706, in launch_core_engines
    wait_for_engine_startup(
  File "/workspace/vllm_hope/vllm/vllm/v1/engine/utils.py", line 759, in wait_for_engine_startup
    raise RuntimeError("Engine core initialization failed. "
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
signal_runner is shutting down
Comms Runner shutting down
Cache Watcher shutting down
hsaInterceptor: Application elapsed usecs: 38475758us
